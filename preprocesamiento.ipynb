{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kernel -> py311ml\n",
    "# imports aqui ->\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import auc, roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import shap\n",
    "from shap import SamplingExplainer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, StratifiedShuffleSplit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeramente vamos a leer el archivo y comprobar las variables, con sus tipos de dato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ Shape ------------\n",
      "(32561, 15)\n",
      "------------ Types ------------\n",
      "age                int64\n",
      "workclass         object\n",
      "final-weight       int64\n",
      "education         object\n",
      "education-num      int64\n",
      "marital-status    object\n",
      "occupation        object\n",
      "relationship      object\n",
      "race              object\n",
      "sex               object\n",
      "capital-gain       int64\n",
      "capital-loss       int64\n",
      "hours-per-week     int64\n",
      "native-country    object\n",
      "income            object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('src/adult.data', header=None, sep=',\\s', na_values=[\"?\"], engine='python')\n",
    "data.columns = ['age', 'workclass', 'final-weight', 'education', 'education-num', 'marital-status',\n",
    "                 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss',\n",
    "                   'hours-per-week', 'native-country', 'income']\n",
    "\n",
    "\n",
    "def check_df(dataframe, head=5):\n",
    "    print(\"------------ Shape ------------\")\n",
    "    print(dataframe.shape)\n",
    "    print(\"------------ Types ------------\")\n",
    "    print(dataframe.dtypes)\n",
    "    \n",
    "check_df(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprobamos que variables contienen valores perdidos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora con ayuda de graficas, visualizamos la distribucion de las variables anteriores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def check_col_plt(df, columns):\n",
    "#     num_plots = len(columns)\n",
    "#     # Definimos el número de filas y columnas del grid. En este ejemplo usamos 2x2.\n",
    "#     rows, cols = 2, 2\n",
    "\n",
    "#     fig, axes = plt.subplots(rows, cols, figsize=(12, 8))\n",
    "#     axes = axes.flatten()  # Aplanamos la matriz de ejes para iterar fácilmente\n",
    "\n",
    "#     for i, col in enumerate(columns):\n",
    "#         ax = axes[i]\n",
    "#         # Obtenemos la cuenta de cada valor único en la columna\n",
    "#         count_data = df[col].value_counts().reset_index()\n",
    "#         count_data.columns = [col, 'count']\n",
    "\n",
    "#         # Creamos el gráfico de barras\n",
    "#         bars = ax.bar(count_data[col].astype(str), count_data['count'], color='skyblue')\n",
    "#         ax.set_title(f\"Distribución de {col}\")\n",
    "#         ax.set_xlabel(col)\n",
    "#         ax.set_ylabel(\"Conteo\")\n",
    "\n",
    "#         # Añadimos las etiquetas de conteo encima de cada barra\n",
    "#         for bar in bars:\n",
    "#             height = bar.get_height()\n",
    "#             ax.text(bar.get_x() + bar.get_width()/2, height, f'{int(height)}',\n",
    "#                     ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "#     # Eliminamos los ejes sobrantes si hay menos plots que subplots\n",
    "#     for j in range(num_plots, len(axes)):\n",
    "#         fig.delaxes(axes[j])\n",
    "    \n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "# print(data.columns)\n",
    "# # Ejemplo de uso\n",
    "# ncolumns = [\"final-weight\", \"education\", \"education-num\"]\n",
    "# check_col_plt(data, ncolumns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OUTLIERS CON MEDIANA\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class OutlierDetecion_treatment_IQR(TransformerMixin):\n",
    "\n",
    "    # Constructor de la clase\n",
    "    def __init__(self, k=1.5, columns=None):\n",
    "        self.k = k\n",
    "        self.columns = columns\n",
    "    \n",
    "    # Método fit\n",
    "    def fit(self, X, y=None):\n",
    "        # Transformamos X a DataFrame por si llega un array de Numpy (para compatibilidad en la Pipeline)\n",
    "        X = pd.DataFrame(X)\n",
    "        if self.columns == None:\n",
    "            # Si no se determinan variables en el constructor si tratan todas\n",
    "            self.columns = X.columns\n",
    "        self.stats = X.describe(percentiles=[0.25, 0.75])\n",
    "        # Devolvemos el propio objeto modificado\n",
    "        return self\n",
    "\n",
    "    # Método transform\n",
    "    def transform(self, X):\n",
    "        # Transformamos X a DataFrame por si llega un array de Numpy (para compatibilidad en la Pipeline)\n",
    "        X = pd.DataFrame(X)\n",
    "        # Creamos una copia del DataFrame X para no perder los datos originales\n",
    "        Xaux = X.copy()\n",
    "        # Se calcula el IQR de cada variable\n",
    "        # IQRs = Xaux.quantile(0.75) - Xaux.quantile(0.25)\n",
    "        IQRs = self.stats.loc[\"75%\", :] - self.stats.loc[\"25%\", :]\n",
    "        # Se calculan los límites inferiores y superiores   \n",
    "        limiteInf = self.stats.loc[\"25%\", :] - self.k * IQRs\n",
    "        limiteSup = self.stats.loc[\"75%\", :] + self.k * IQRs\n",
    "        # Se comprueba qué elementos están por encima y por debajo de dichos límites (máscaras de booleanos)  \n",
    "        menores = Xaux < limiteInf\n",
    "        mayores = Xaux > limiteSup\n",
    "        valores = np.logical_or(menores, mayores)\n",
    "        # Se recorren las variables para detectar outliers y tratarlos (sustituir por la mediana de la variable)\n",
    "        out_total = 0\n",
    "        for c in self.columns:\n",
    "            # obtenemos la lista de booleanos correspondientes a si los valores de los ejemplos son outliers o no para la variable c\n",
    "            indices = valores[c]\n",
    "            # Si hay outliers\n",
    "            if indices.any():\n",
    "                # Los sustituimos por la mediana\n",
    "                Xaux.loc[indices, c] = Xaux[c].median()\n",
    "                out_total += len(indices)\n",
    "        # Se devuelve el DataFrame modificado\n",
    "        # print(\"Total de outliers: \", out_total)\n",
    "        return Xaux\n",
    "    \n",
    "    # Método para asignar los valores de los híper-parámetros y que, de este modo, \n",
    "        # podamos aplicar GridSearchCV sobre un objeto de esta clase\n",
    "    def set_params(self, **parameters):\n",
    "        for parameter, value in parameters.items():\n",
    "            setattr(self, parameter, value)\n",
    "        return self\n",
    "    \n",
    "    # Método para obtener los valores de los híper-parámetros que queramos del modelo (lo usa GridSearchCV al mostrar la mejor configuración)\n",
    "    def get_params(self, deep=True):\n",
    "        # Devolvemos los valores de los híper-parámetros del método de preparación de datos\n",
    "        return {\"k\": self.k}\n",
    "    \n",
    "class OutlierDetecion_treatment_MeanStd(TransformerMixin):\n",
    "    # Constructor de la clase\n",
    "    def __init__(self, k=2):\n",
    "        self.k = k\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = pd.DataFrame(X)\n",
    "        self.mean = X.mean()\n",
    "        self.std = X.std()\n",
    "        self.median = X.median()\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        Xaux = X.copy()\n",
    "        out_total = 0\n",
    "        for c in X.columns:\n",
    "            # Se calculan los límites inferiores y superiores\n",
    "            limiteInf = self.mean[c] - self.k * self.std[c]\n",
    "            limiteSup = self.mean[c] + self.k * self.std[c]\n",
    "            # Se comprueba qué elementos están por encima y por debajo de dichos límites (máscaras de booleanos)\n",
    "            menores = Xaux[c] < limiteInf\n",
    "            mayores = Xaux[c] > limiteSup\n",
    "            valores = np.logical_or(menores, mayores)\n",
    "            # Si hay outliers\n",
    "            if valores.any():\n",
    "                # Los sustituimos por la mediana\n",
    "                Xaux.loc[valores, c] = self.median[c]\n",
    "                out_total += len(valores)\n",
    "        # print(\"Total de outliers: \" , out_total)\n",
    "        return Xaux\n",
    "    \n",
    "    def set_params(self, **parameters):\n",
    "        for parameter, value in parameters.items():\n",
    "            setattr(self, parameter, value)\n",
    "        return self\n",
    "    def get_params(self, deep=True):\n",
    "        return {\"k\": self.k}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OUTLIERS CON MEDIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class OutlierDetecion_treatment_IQR(TransformerMixin):\n",
    "\n",
    "    # Constructor de la clase\n",
    "    def __init__(self, k=1.5, columns=None):\n",
    "        self.k = k\n",
    "        self.columns = columns\n",
    "    \n",
    "    # Método fit\n",
    "    def fit(self, X, y=None):\n",
    "        # Transformamos X a DataFrame por si llega un array de Numpy (para compatibilidad en la Pipeline)\n",
    "        X = pd.DataFrame(X)\n",
    "        if self.columns == None:\n",
    "            # Si no se determinan variables en el constructor si tratan todas\n",
    "            self.columns = X.columns\n",
    "        self.stats = X.describe(percentiles=[0.25, 0.75])\n",
    "        # Devolvemos el propio objeto modificado\n",
    "        return self\n",
    "\n",
    "    # Método transform\n",
    "    def transform(self, X):\n",
    "        # Transformamos X a DataFrame por si llega un array de Numpy (para compatibilidad en la Pipeline)\n",
    "        X = pd.DataFrame(X)\n",
    "        # Creamos una copia del DataFrame X para no perder los datos originales\n",
    "        Xaux = X.copy()\n",
    "        # Se calcula el IQR de cada variable\n",
    "        # IQRs = Xaux.quantile(0.75) - Xaux.quantile(0.25)\n",
    "        IQRs = self.stats.loc[\"75%\", :] - self.stats.loc[\"25%\", :]\n",
    "        # Se calculan los límites inferiores y superiores   \n",
    "        limiteInf = self.stats.loc[\"25%\", :] - self.k * IQRs\n",
    "        limiteSup = self.stats.loc[\"75%\", :] + self.k * IQRs\n",
    "        # Se comprueba qué elementos están por encima y por debajo de dichos límites (máscaras de booleanos)  \n",
    "        menores = Xaux < limiteInf\n",
    "        mayores = Xaux > limiteSup\n",
    "        valores = np.logical_or(menores, mayores)\n",
    "        # Se recorren las variables para detectar outliers y tratarlos (sustituir por la mediana de la variable)\n",
    "        out_total = 0\n",
    "        for c in self.columns:\n",
    "            # obtenemos la lista de booleanos correspondientes a si los valores de los ejemplos son outliers o no para la variable c\n",
    "            indices = valores[c]\n",
    "            # Si hay outliers\n",
    "            if indices.any():\n",
    "                # Los sustituimos por la mediana\n",
    "                Xaux.loc[indices, c] = Xaux[c].mean()\n",
    "                out_total += len(indices)\n",
    "        # Se devuelve el DataFrame modificado\n",
    "        # print(\"Total de outliers: \", out_total)\n",
    "        return Xaux\n",
    "    \n",
    "    # Método para asignar los valores de los híper-parámetros y que, de este modo, \n",
    "        # podamos aplicar GridSearchCV sobre un objeto de esta clase\n",
    "    def set_params(self, **parameters):\n",
    "        for parameter, value in parameters.items():\n",
    "            setattr(self, parameter, value)\n",
    "        return self\n",
    "    \n",
    "    # Método para obtener los valores de los híper-parámetros que queramos del modelo (lo usa GridSearchCV al mostrar la mejor configuración)\n",
    "    def get_params(self, deep=True):\n",
    "        # Devolvemos los valores de los híper-parámetros del método de preparación de datos\n",
    "        return {\"k\": self.k}\n",
    "    \n",
    "class OutlierDetecion_treatment_MeanStd(TransformerMixin):\n",
    "    # Constructor de la clase\n",
    "    def __init__(self, k=2):\n",
    "        self.k = k\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = pd.DataFrame(X)\n",
    "        self.mean = X.mean()\n",
    "        self.std = X.std()\n",
    "        self.median = X.median()\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        Xaux = X.copy()\n",
    "        out_total = 0\n",
    "        for c in X.columns:\n",
    "            # Se calculan los límites inferiores y superiores\n",
    "            limiteInf = self.mean[c] - self.k * self.std[c]\n",
    "            limiteSup = self.mean[c] + self.k * self.std[c]\n",
    "            # Se comprueba qué elementos están por encima y por debajo de dichos límites (máscaras de booleanos)\n",
    "            menores = Xaux[c] < limiteInf\n",
    "            mayores = Xaux[c] > limiteSup\n",
    "            valores = np.logical_or(menores, mayores)\n",
    "            # Si hay outliers\n",
    "            if valores.any():\n",
    "                # Los sustituimos por la mediana\n",
    "                Xaux.loc[valores, c] = self.mean[c]\n",
    "                out_total += len(valores)\n",
    "        # print(\"Total de outliers: \" , out_total)\n",
    "        return Xaux\n",
    "    \n",
    "    def set_params(self, **parameters):\n",
    "        for parameter, value in parameters.items():\n",
    "            setattr(self, parameter, value)\n",
    "        return self\n",
    "    def get_params(self, deep=True):\n",
    "        return {\"k\": self.k}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OUTLIERS CON LIMITE SUPERIOR - INFERIOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutlierDetecion_treatment_IQR(TransformerMixin):\n",
    "\n",
    "    # Constructor de la clase\n",
    "    def __init__(self, k=1.5, columns=None):\n",
    "        self.k = k\n",
    "        self.columns = columns\n",
    "    \n",
    "    # Método fit\n",
    "    def fit(self, X, y=None):\n",
    "        # Transformamos X a DataFrame por si llega un array de Numpy (para compatibilidad en la Pipeline)\n",
    "        X = pd.DataFrame(X)\n",
    "        if self.columns == None:\n",
    "            # Si no se determinan variables en el constructor si tratan todas\n",
    "            self.columns = X.columns\n",
    "        self.stats = X.describe(percentiles=[0.25, 0.75])\n",
    "        # Devolvemos el propio objeto modificado\n",
    "        return self\n",
    "\n",
    "    # Método transform\n",
    "    def transform(self, X):\n",
    "        X = pd.DataFrame(X)\n",
    "        Xaux = X.copy()\n",
    "\n",
    "        # Estadísticos guardados en fit\n",
    "        IQRs      = self.stats.loc[\"75%\", :] - self.stats.loc[\"25%\", :]\n",
    "        limiteInf = self.stats.loc[\"25%\", :] - self.k * IQRs\n",
    "        limiteSup = self.stats.loc[\"75%\", :] + self.k * IQRs\n",
    "\n",
    "        # Para cada columna, winsorizamos\n",
    "        for c in self.columns:\n",
    "            mask_low  = Xaux[c] < limiteInf[c]\n",
    "            mask_high = Xaux[c] > limiteSup[c]\n",
    "            if mask_low.any():\n",
    "                Xaux.loc[mask_low,  c] = limiteInf[c]\n",
    "            if mask_high.any():\n",
    "                Xaux.loc[mask_high, c] = limiteSup[c]\n",
    "\n",
    "        return Xaux\n",
    "    \n",
    "    # Método para asignar los valores de los híper-parámetros y que, de este modo, \n",
    "        # podamos aplicar GridSearchCV sobre un objeto de esta clase\n",
    "    def set_params(self, **parameters):\n",
    "        for parameter, value in parameters.items():\n",
    "            setattr(self, parameter, value)\n",
    "        return self\n",
    "    \n",
    "    # Método para obtener los valores de los híper-parámetros que queramos del modelo (lo usa GridSearchCV al mostrar la mejor configuración)\n",
    "    def get_params(self, deep=True):\n",
    "        # Devolvemos los valores de los híper-parámetros del método de preparación de datos\n",
    "        return {\"k\": self.k}\n",
    "    \n",
    "    from sklearn.base import TransformerMixin\n",
    "\n",
    "class OutlierDetecion_treatment_MeanStd(TransformerMixin):\n",
    "    def __init__(self, k=2):\n",
    "        self.k = k\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = pd.DataFrame(X)\n",
    "        # Calculamos y guardamos media y desviación por columna\n",
    "        self.mean_ = X.mean()\n",
    "        self.std_  = X.std()\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        Xaux = pd.DataFrame(X).copy()\n",
    "        # Cálculo de límites por columna\n",
    "        limiteInf = self.mean_ - self.k * self.std_\n",
    "        limiteSup = self.mean_ + self.k * self.std_\n",
    "        \n",
    "        # Winsorización\n",
    "        for c in Xaux.columns:\n",
    "            # Máscaras\n",
    "            mask_low  = Xaux[c] < limiteInf[c]\n",
    "            mask_high = Xaux[c] > limiteSup[c]\n",
    "            # Sustitución por límites\n",
    "            if mask_low.any():\n",
    "                Xaux.loc[mask_low,  c] = limiteInf[c]\n",
    "            if mask_high.any():\n",
    "                Xaux.loc[mask_high, c] = limiteSup[c]\n",
    "        \n",
    "        return Xaux\n",
    "    \n",
    "    def set_params(self, **parameters):\n",
    "        for parameter, value in parameters.items():\n",
    "            setattr(self, parameter, value)\n",
    "        return self\n",
    "    \n",
    "    def get_params(self, deep=True):\n",
    "        return {\"k\": self.k}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version para usar con column transformer y meter este paso en la pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rellenarModa(data):\n",
    "    dataCopiada = data.copy()\n",
    "    # Si 'columnas' no es una lista, se convierte en una lista\n",
    "    columnas = dataCopiada.columns[dataCopiada.isnull().any()].tolist()\n",
    "    for columna in columnas:\n",
    "        if (columna != \"occupation\"):\n",
    "        # Calcula la moda de la columna y rellena los valores nulos\n",
    "            moda = dataCopiada[columna].mode(dropna = True)\n",
    "            dataCopiada[columna] = dataCopiada[columna].fillna(moda)\n",
    "    return dataCopiada\n",
    "\n",
    "imputador_modas = FunctionTransformer(rellenarModa, validate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rellenarKnn(data):\n",
    "    #Columna a imputar\n",
    "    columna = \"occupation\"\n",
    "\n",
    "    # 2. Crear una copia del DataFrame\n",
    "    dataCopiada = data.copy()\n",
    "\n",
    "    # 3. Guardar las filas con y sin valores faltantes\n",
    "    #df_completo = df_copy[df_copy[columna].notna()]\n",
    "    #df_faltante = df_copy[df_copy[columna].isna()]\n",
    "\n",
    "    # 4. Codificar la columna objetivo con LabelEncoder\n",
    "    le_obj = LabelEncoder()\n",
    "    dataCopiada[columna] = dataCopiada[columna].astype(str)\n",
    "    dataCopiada[columna] = le_obj.fit_transform(dataCopiada[columna])\n",
    "\n",
    "    # 5. Codificar otras columnas categóricas (si hay), necesarias para el imputador\n",
    "    label_encoders = {}\n",
    "    for col in dataCopiada.columns:\n",
    "        if dataCopiada[col].dtype == 'object' and col != columna:\n",
    "            le = LabelEncoder()\n",
    "            dataCopiada[col] = dataCopiada[col].astype(str)\n",
    "            dataCopiada[col] = le.fit_transform(dataCopiada[col])\n",
    "            label_encoders[col] = le\n",
    "\n",
    "# 6. Crear subconjunto de columnas para imputación\n",
    "# (puede ser todo menos la columna objetivo o solo algunas relevantes)\n",
    "    columnasPred = dataCopiada.columns[dataCopiada.columns != columna]\n",
    "\n",
    "    # 7. Juntar columna objetivo + predictoras\n",
    "    df_model = dataCopiada[[columna] + list(columnasPred)]\n",
    "\n",
    "    # 8. Aplicar KNNImputer solo a esta matriz\n",
    "    imputer = KNNImputer(n_neighbors=5)\n",
    "    dataImputada = pd.DataFrame(imputer.fit_transform(df_model), columns=df_model.columns)\n",
    "\n",
    "    # 9. Redondear y reconvertir la columna objetivo\n",
    "    dataImputada[columna] = dataImputada[columna].astype(int)\n",
    "    dataImputada[columna] = le_obj.inverse_transform(dataImputada[columna])\n",
    "\n",
    "    # 10. Insertar la columna imputada en el DataFrame original\n",
    "    dataFinal = data.copy()\n",
    "    dataFinal[columna] = dataImputada[columna]\n",
    "    # print(dataFinal.isnull().sum())\n",
    "    return dataFinal\n",
    "\n",
    "imputador_knn = FunctionTransformer(rellenarKnn, validate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7960687960687961\n",
      "Confusion Matrix:\n",
      " [[15375  4420]\n",
      " [  892  5361]]\n",
      "AUC: 0.8170298817315352\n",
      "AUC en Val : 0.7839083097853188\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "def transformar_income(X): \n",
    "    \"\"\"Transforma la columna 'income' en una variable binaria.\"\"\"\n",
    "\n",
    "    X = X.copy()\n",
    "    X['income'] = X['income'].str.strip().map({'<=50K': 0, '>50K': 1})\n",
    "    return X\n",
    "\n",
    "def separar_x_y(data):\n",
    "    X = data.drop('income', axis=1)\n",
    "    y = data['income']\n",
    "    return X, y\n",
    "\n",
    "def separar_x_y_col(data, col):\n",
    "    X = data.drop(col, axis=1)\n",
    "    y = data[col]\n",
    "    return X, y\n",
    "\n",
    "def transformar_categoricas_a_binarias(X): \n",
    "\n",
    "    X = X.copy()\n",
    "    variables_categoricas_binarias = X.select_dtypes(include=['object']).columns\n",
    "    dummies = pd.get_dummies(X, columns=variables_categoricas_binarias)\n",
    "    if hasattr(transformar_categoricas_a_binarias, 'dummy_columns'):\n",
    "        dummies = dummies.reindex(columns=transformar_categoricas_a_binarias.dummy_columns, fill_value=0)\n",
    "    else:\n",
    "        transformar_categoricas_a_binarias.dummy_columns = dummies.columns\n",
    "    return dummies\n",
    "\n",
    "def knnImp(X, y):\n",
    "    y = y.copy()\n",
    "    indicesNulos = np.where(y == -1)[0] \n",
    "    indicesNoNulos = np.where(y != -1)[0] \n",
    "    X_train = X.iloc[indicesNoNulos]\n",
    "    X_test = X.iloc[indicesNulos]\n",
    "    y_train = y[indicesNoNulos]\n",
    "\n",
    "    \n",
    "    knn = KNeighborsClassifier(n_neighbors= 3)\n",
    "    knn.fit(X_train, y_train)\n",
    "    pred = knn.predict(X_test)\n",
    "    y[indicesNulos] = pred\n",
    "    return y\n",
    "\n",
    "def eliminar_columna(data, col):\n",
    "    if not isinstance(data, pd.DataFrame):\n",
    "        data = pd.DataFrame(data)\n",
    "    return data.drop(columns=[col], errors=\"ignore\")\n",
    "\n",
    "binary_transformer = FunctionTransformer(transformar_categoricas_a_binarias)\n",
    "eliminar_relationship = FunctionTransformer(eliminar_columna, kw_args={\"col\": [\"relationship\"]}, validate=False)\n",
    "eliminar_marital_status = FunctionTransformer(eliminar_columna, kw_args={\"col\": [\"marital-status\"]}, validate=False)\n",
    "eliminar_education = FunctionTransformer(eliminar_columna, kw_args={\"col\": [\"education\"]}, validate=False)\n",
    "\n",
    "\n",
    "data_binary_income = transformar_income(data)\n",
    "X, X_val = train_test_split(data_binary_income, test_size=0.2, random_state=34)\n",
    "X, y = separar_x_y(X)\n",
    "X_val, y_val = separar_x_y(X_val)\n",
    "\n",
    "# X, occupation = separar_x_y_col(X, \"occupation\")\n",
    "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_features = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "# X_transformed = binary_transformer.transform(X)\n",
    "# occupationNombre = occupation.copy()\n",
    "# valoresOccupation = occupationNombre.dropna()\n",
    "# colConNulos = pd.Categorical(occupation, ordered= True).codes #Paso la variable a ordinal \n",
    "\n",
    "# occupation_ord = knnImp(X_transformed, colConNulos) #Imputo los valores\n",
    "# occupation_series = pd.Series(occupation_ord)\n",
    "# occupation_mapeados = occupation_series.map(lambda x: occupationNombre[x])\n",
    "\n",
    "# occupationOHE = pd.get_dummies(occupation_mapeados)\n",
    "# X_transformed = pd.concat([X_transformed, occupationOHE], axis = 1) #Todas las columnas sin nulos y en OHE\n",
    "\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    # ('outlier_detection', OutlierDetecion_treatment_MeanStd(k=3)),\n",
    "    ('outlier_detection', OutlierDetecion_treatment_IQR(k=5)),\n",
    "    # ('scaler', RobustScaler())\n",
    "    # ('scaler', StandardScaler()), \n",
    "    # ('scaler', MinMaxScaler()) Descartado despues de las pruebas\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('eliminar_variable_relationship', eliminar_relationship),\n",
    "    # ('eliminar_variable_relationship', eliminar_education),\n",
    "    ('one_hot_encoding', OneHotEncoder())\n",
    "])\n",
    "\n",
    "preprocessorBasic = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features), \n",
    "        ('cat', categorical_transformer, categorical_features),\n",
    "    ])\n",
    "\n",
    "# Definimos el pipeline con el transformador\n",
    "pipeline = Pipeline([\n",
    "    ('valores_perdidos', imputador_modas),\n",
    "    # ('ocupation_perdidos', imputador_knn),\n",
    "    ('preprocesador', preprocessorBasic),\n",
    "    # ('classifier', KNeighborsClassifier())\n",
    "    ('classifier', DecisionTreeClassifier())\n",
    "])\n",
    "\n",
    "pipeline.fit(X, y)\n",
    "predicciones = pipeline.predict(X)\n",
    "print(\"Accuracy:\", accuracy_score(y, predicciones))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y, predicciones))\n",
    "print(\"AUC:\", roc_auc_score(y, predicciones))\n",
    "print(\"AUC en Val :\", roc_auc_score(y_val, pipeline.predict(X_val))) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GridSearch CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 5400 candidates, totalling 5400 fits\n",
      "Mejores hiperparámetros:\n",
      "{'classifier__class_weight': 'balanced', 'classifier__criterion': 'entropy', 'classifier__max_depth': 20, 'classifier__max_features': 'sqrt', 'classifier__min_impurity_decrease': 0.0, 'classifier__min_samples_leaf': 4, 'classifier__min_samples_split': 50, 'classifier__min_weight_fraction_leaf': 0.0}\n",
      "Mejor media de CV: 0.8706\n"
     ]
    }
   ],
   "source": [
    "param_grid_knn = {\n",
    "    'classifier__n_neighbors' : [10, 50, 100],\n",
    "    'classifier__weights'     : ['uniform', 'distance'],\n",
    "    'classifier__metric'      : ['euclidean', 'manhattan'],\n",
    "}\n",
    "param_grid_tree = {\n",
    "    'classifier__max_depth' : [10, 20, 30],\n",
    "    'classifier__min_samples_split' : [10, 20, 50],\n",
    "    'classifier__min_samples_leaf' : [1, 2, 4],\n",
    "    'classifier__max_features' : ['sqrt', 'log2'],\n",
    "    'classifier__criterion' : ['gini', 'entropy'],\n",
    "    'classifier__class_weight' : ['balanced', None],\n",
    "    'classifier__min_weight_fraction_leaf' : [0.0, 0.1, 0.2],\n",
    "    # 'classifier__ccp_alpha' : [0.0, 0.01, 0.1],\n",
    "    'classifier__min_impurity_decrease' : [0.0, 0.1, 0.2],    \n",
    "}    \n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 3.  Configuración del GridSearch ------------------------------------\n",
    "# --------------------------------------------------------------------\n",
    "holdout = StratifiedShuffleSplit(\n",
    "    n_splits=1, test_size=0.2, random_state=42)  # 80 % / 20 %\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=param_grid_tree,\n",
    "    # param_grid=param_grid_knn,\n",
    "    cv=holdout,\n",
    "    scoring='roc_auc',         # o 'roc_auc', 'balanced_accuracy', etc.\n",
    "    n_jobs=-1,                  # paraliza en todos los cores disponibles\n",
    "    verbose=2,                  # para ver progreso\n",
    ")\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 4.  Ajuste -----------------------------------------------------------\n",
    "# --------------------------------------------------------------------\n",
    "data_binary_income = transformar_income(data)\n",
    "X, y = separar_x_y(data_binary_income)\n",
    "\n",
    "grid.fit(X, y)\n",
    "\n",
    "print(\"Mejores hiperparámetros:\")\n",
    "print(grid.best_params_)\n",
    "print(f\"Mejor media de CV: {grid.best_score_:.4f}\")\n",
    "\n",
    "# El modelo entrenado con los mejores parámetros:\n",
    "best_model = grid.best_estimator_\n",
    "\n",
    "best = grid.best_params_\n",
    "clf_params = {\n",
    "    key.split('__', 1)[1]: value\n",
    "    for key, value in best.items()\n",
    "    if key.startswith('classifier__')\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from scipy.stats import chi2_contingency\n",
    "\n",
    "# for c in X_transformed.columns:\n",
    "#     if c in X:  # No es OHE\n",
    "#         print(\"\")\n",
    "#     else:\n",
    "#         contingency_table = pd.crosstab(X_transformed[c], y)  # Tabla de frecuencias\n",
    "\n",
    "#         chi2, _, _, _ = chi2_contingency(contingency_table)  \n",
    "#         n = contingency_table.sum().sum()  \n",
    "#         r, k = contingency_table.shape  \n",
    "\n",
    "#         if min(r, k) - 1 == 0: #Por si hay una division por cero\n",
    "#             print(\"Fallo en \", c, \" división por 0\")\n",
    "#             continue\n",
    "#         q = min(r, k)\n",
    "#         v = np.sqrt(chi2 / (n * ( q - 1)))  \n",
    "#         if (v > 0.1):\n",
    "#             print(\"La correlacion para \", c, \"es de \", v)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez pasadas todas las variables a numericas, podemos ver la matriz de corelacion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_transformed = binary_transformer.transform(X)\n",
    "# X_transformed = pd.concat([X_transformed, y], axis=1)\n",
    "# corr_matrix = X_transformed.corr()\n",
    "# filtered_corr = corr_matrix.where(corr_matrix != 1)\n",
    "\n",
    "# plt.figure(figsize=(60,60))\n",
    "# sns.heatmap(filtered_corr, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observan varias correlaciones, algunas obvias como marital-status_Married-civ-spouse con relationship_husband y otras que vamos a analizar. Primero vamos a ver las mas influyentes en el propio resultado\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlations = X_transformed.corr()['income'].sort_values(ascending=False)\n",
    "# correlations = abs(correlations).sort_values(ascending=False)\n",
    "# correlations = correlations.drop('relationship_Husband') \n",
    "# correlations = correlations.drop('sex_Male')\n",
    "# correlations = correlations.drop('sex_Female')\n",
    "\n",
    "\n",
    "# print(\"Most influential factors:\\n\", correlations[:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probamos el rendimiento de nuetro modelo con la metrica seleccionada "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9999385768250361\n",
      "Confusion Matrix:\n",
      " [[24720     0]\n",
      " [    2  7839]]\n",
      "AUC: 0.9998724652467798\n"
     ]
    }
   ],
   "source": [
    "predicciones = pipeline.predict(X)\n",
    "print(\"Accuracy:\", accuracy_score(y, predicciones))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y, predicciones))\n",
    "print(\"AUC:\", roc_auc_score(y, predicciones)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probamos el rendimiento de nuestro modelo con el test.data que previamente esta dividio por|1x3 Cross validator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Accuracy: 0.7770407223143542\n",
      "Test Confusion Matrix:\n",
      " [[10612  1823]\n",
      " [ 1807  2039]]\n",
      "Test AUC: 0.6917794371605985\n"
     ]
    }
   ],
   "source": [
    "#Quitamos el punto final\n",
    "def transformar_income_test(X): \n",
    "    X = X.copy()\n",
    "    X['income'] = X['income'].str.strip().str.rstrip('.')\n",
    "    X['income'] = X['income'].map({'<=50K': 0, '>50K': 1})\n",
    "    return X\n",
    "\n",
    "data_test = pd.read_csv('src/adult.test', header=None, skiprows=1, sep=',\\s', na_values=[\"?\"], engine='python')\n",
    "data_test.columns = ['age', 'workclass', 'final-weight', 'education', 'education-num', 'marital-status',\n",
    "                     'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss',\n",
    "                     'hours-per-week', 'native-country', 'income']\n",
    "\n",
    "data_test = transformar_income_test(data_test)\n",
    "X_test, y_test = separar_x_y(data_test)\n",
    "# X_test_transformed = binary_transformer.transform(X_test)\n",
    "\n",
    "\n",
    "#Predecimos los valores con el conjunto del test\n",
    "predicciones_test = pipeline.predict(X_test)\n",
    "print(\"\\nTest Accuracy:\", accuracy_score(y_test, predicciones_test))\n",
    "print(\"Test Confusion Matrix:\\n\", confusion_matrix(y_test, predicciones_test))\n",
    "print(\"Test AUC:\", roc_auc_score(y_test, predicciones_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SHAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Globales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. DataFrames de referencia\n",
    "feature_names = X.columns\n",
    "\n",
    "# 2. Función envoltorio\n",
    "def model_predict(data):\n",
    "    \"\"\"Asegura que el Pipeline recibe un DataFrame con los nombres correctos.\"\"\"\n",
    "    if not isinstance(data, pd.DataFrame):\n",
    "        data = pd.DataFrame(data, columns=feature_names)\n",
    "    return pipeline.predict_proba(data)[:, 1]\n",
    "\n",
    "# 3. Background y muestra de evaluación\n",
    "background = shap.sample(X, 200, random_state=1)     # o shap.kmeans(X, 100)\n",
    "X_eval     = shap.sample(X, 500, random_state=1)\n",
    "\n",
    "# 4. Explainer (Kernel o Sampling)\n",
    "explainer   = shap.KernelExplainer(model_predict, background)\n",
    "# explainer = shap.SamplingExplainer(model_predict, background)  # algo más rápido\n",
    "\n",
    "shap_values = explainer.shap_values(X_eval, nsamples=200)\n",
    "\n",
    "# 5. Gráfico\n",
    "shap.summary_plot(shap_values, X_eval)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Individuales "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. Solo una vez por notebook\n",
    "shap.initjs()\n",
    "\n",
    "# 1. Wrapper para el pipeline  ─ convierte ndarray→DataFrame si hace falta\n",
    "feature_names = X.columns          # guarda los nombres originales\n",
    "\n",
    "def model_predict(data):\n",
    "    if not isinstance(data, pd.DataFrame):\n",
    "        data = pd.DataFrame(data, columns=feature_names)\n",
    "    return pipeline.predict_proba(data)\n",
    "\n",
    "# 2. Background pequeño (muestra o k‑means)\n",
    "background = shap.sample(X, 200, random_state=0)      # o shap.kmeans(X, 100)\n",
    "\n",
    "# 3. Explainer (Kernel o Sampling) usando la *función*, no el pipeline\n",
    "explainer = shap.KernelExplainer(model_predict, background)\n",
    "# explainer = shap.SamplingExplainer(model_predict, background)  # 10 × más rápido\n",
    "\n",
    "# 4. Fila a explicar\n",
    "instance = X.iloc[[1]]             # DataFrame de 1 fila\n",
    "\n",
    "# 5. Cálculo limitado de coaliciones\n",
    "shap_values = explainer.shap_values(instance, nsamples=200)\n",
    "\n",
    "# 6. Force plot (clase positiva = 1)\n",
    "shap.force_plot(\n",
    "    explainer.expected_value[1],\n",
    "    shap_values[0,:,0],             # clase 1, primera fila\n",
    "    instance\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

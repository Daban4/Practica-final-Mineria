{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kernel -> py311ml\n",
    "# imports aqui ->\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import auc, roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeramente vamos a leer el archivo y comprobar las variables, con sus tipos de dato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ Shape ------------\n",
      "(32561, 15)\n",
      "------------ Types ------------\n",
      "age                int64\n",
      "workclass         object\n",
      "final-weight       int64\n",
      "education         object\n",
      "education-num      int64\n",
      "marital-status    object\n",
      "occupation        object\n",
      "relationship      object\n",
      "race              object\n",
      "sex               object\n",
      "capital-gain       int64\n",
      "capital-loss       int64\n",
      "hours-per-week     int64\n",
      "native-country    object\n",
      "income            object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('src/adult.data', header=None, sep=',\\s', na_values=[\"?\"], engine='python')\n",
    "data.columns = ['age', 'workclass', 'final-weight', 'education', 'education-num', 'marital-status',\n",
    "                 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss',\n",
    "                   'hours-per-week', 'native-country', 'income']\n",
    "\n",
    "\n",
    "def check_df(dataframe, head=5):\n",
    "    print(\"------------ Shape ------------\")\n",
    "    print(dataframe.shape)\n",
    "    print(\"------------ Types ------------\")\n",
    "    print(dataframe.dtypes)\n",
    "    \n",
    "check_df(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprobamos que variables contienen valores perdidos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age                  0\n",
       "workclass         1836\n",
       "final-weight         0\n",
       "education            0\n",
       "education-num        0\n",
       "marital-status       0\n",
       "occupation        1843\n",
       "relationship         0\n",
       "race                 0\n",
       "sex                  0\n",
       "capital-gain         0\n",
       "capital-loss         0\n",
       "hours-per-week       0\n",
       "native-country     583\n",
       "income               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora con ayuda de graficas, visualizamos la distribucion de las variables anteriores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def check_col_plt(df, columns):\n",
    "    num_plots = len(columns)\n",
    "    # Definimos el número de filas y columnas del grid. En este ejemplo usamos 2x2.\n",
    "    rows, cols = 2, 2\n",
    "\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(12, 8))\n",
    "    axes = axes.flatten()  # Aplanamos la matriz de ejes para iterar fácilmente\n",
    "\n",
    "    for i, col in enumerate(columns):\n",
    "        ax = axes[i]\n",
    "        # Obtenemos la cuenta de cada valor único en la columna\n",
    "        count_data = df[col].value_counts().reset_index()\n",
    "        count_data.columns = [col, 'count']\n",
    "\n",
    "        # Creamos el gráfico de barras\n",
    "        bars = ax.bar(count_data[col].astype(str), count_data['count'], color='skyblue')\n",
    "        ax.set_title(f\"Distribución de {col}\")\n",
    "        ax.set_xlabel(col)\n",
    "        ax.set_ylabel(\"Conteo\")\n",
    "\n",
    "        # Añadimos las etiquetas de conteo encima de cada barra\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2, height, f'{int(height)}',\n",
    "                    ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "    # Eliminamos los ejes sobrantes si hay menos plots que subplots\n",
    "    for j in range(num_plots, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "print(data.columns)\n",
    "# Ejemplo de uso\n",
    "ncolumns = [\"final-weight\", \"education\", \"education-num\"]\n",
    "check_col_plt(data, ncolumns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Valores periddos\n",
    "columnas_con_nulos = data.columns[data.isnull().any()].tolist()\n",
    "print(columnas_con_nulos)\n",
    "\n",
    "def rellenarModa(data, columna):\n",
    "    dataCopiada = data.copy()\n",
    "    moda = dataCopiada.loc[:, columna].mode()[0]\n",
    "    dataCopiada[columna] = dataCopiada[columna].fillna(moda)\n",
    "    return dataCopiada\n",
    "\n",
    "X = rellenarModa(data, columnas_con_nulos[0])\n",
    "X = rellenarModa(X, columnas_con_nulos[2])\n",
    "print(X.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version para usar con column transformer y meter este paso en la pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rellenarModa(data):\n",
    "    dataCopiada = data.copy()\n",
    "    # Si 'columnas' no es una lista, se convierte en una lista\n",
    "    columnas = dataCopiada.columns[dataCopiada.isnull().any()].tolist()\n",
    "    for columna in columnas:\n",
    "        print(\"1 columna\")\n",
    "        # Calcula la moda de la columna y rellena los valores nulos\n",
    "        moda = dataCopiada[columna].mode()[0]\n",
    "        dataCopiada[columna] = dataCopiada[columna].fillna(moda)\n",
    "    return dataCopiada\n",
    "# Ahora lo envolvemos en un FunctionTransformer\n",
    "imputador_modas = FunctionTransformer(rellenarModa, validate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformar_income(X): \n",
    "    \"\"\"Transforma la columna 'income' en una variable binaria.\"\"\"\n",
    "\n",
    "    X = X.copy()\n",
    "    X['income'] = X['income'].str.strip().map({'<=50K': 0, '>50K': 1})\n",
    "    return X\n",
    "\n",
    "def separar_x_y(data):\n",
    "    X = data.drop('income', axis=1)\n",
    "    y = data['income']\n",
    "    return X, y\n",
    "\n",
    "def separar_x_y_col(data, col):\n",
    "    X = data.drop(col, axis=1)\n",
    "    y = data[col]\n",
    "    return X, y\n",
    "\n",
    "def transformar_categoricas_a_binarias(X): \n",
    "\n",
    "    X = X.copy()\n",
    "    variables_categoricas_binarias = X.select_dtypes(include=['object']).columns\n",
    "    dummies = pd.get_dummies(X, columns=variables_categoricas_binarias)\n",
    "    if hasattr(transformar_categoricas_a_binarias, 'dummy_columns'):\n",
    "        dummies = dummies.reindex(columns=transformar_categoricas_a_binarias.dummy_columns, fill_value=0)\n",
    "    else:\n",
    "        transformar_categoricas_a_binarias.dummy_columns = dummies.columns\n",
    "    return dummies\n",
    "\n",
    "def knnImp(X, y):\n",
    "    y = y.copy()\n",
    "    indicesNulos = np.where(y == -1)[0] \n",
    "    indicesNoNulos = np.where(y != -1)[0] \n",
    "    X_train = X.iloc[indicesNoNulos]\n",
    "    X_test = X.iloc[indicesNulos]\n",
    "    y_train = y[indicesNoNulos]\n",
    "\n",
    "    \n",
    "    knn = KNeighborsClassifier(n_neighbors= 3)\n",
    "    knn.fit(X_train, y_train)\n",
    "    pred = knn.predict(X_test)\n",
    "    y[indicesNulos] = pred\n",
    "    return y\n",
    "\n",
    "binary_transformer = FunctionTransformer(transformar_categoricas_a_binarias)\n",
    "knn_transformer = FunctionTransformer(knnImp)\n",
    "\n",
    "data_binary_income = transformar_income(data)\n",
    "X, y = separar_x_y(data_binary_income)\n",
    "X, occupation = separar_x_y_col(X, \"occupation\")\n",
    "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_features = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "X_transformed = binary_transformer.transform(X)\n",
    "occupationNombre = occupation.copy()\n",
    "valoresOccupation = occupationNombre.dropna()\n",
    "colConNulos = pd.Categorical(occupation, ordered= True).codes #Paso la variable a ordinal \n",
    "\n",
    "occupation_ord = knnImp(X_transformed, colConNulos) #Imputo los valores\n",
    "occupation_series = pd.Series(occupation_ord)\n",
    "occupation_mapeados = occupation_series.map(lambda x: occupationNombre[x])\n",
    "\n",
    "occupationOHE = pd.get_dummies(occupation_mapeados)\n",
    "X_transformed = pd.concat([X_transformed, occupationOHE], axis = 1) #Todas las columnas sin nulos y en OHE\n",
    "\n",
    "\n",
    "\n",
    "preprocessorBasic = ColumnTransformer(\n",
    "    transformers=[\n",
    "        # ('num', StandardScaler(), numeric_features), mejora un 10% el auc , lo ponemos luego\n",
    "        ('cat', OneHotEncoder(), categorical_features),\n",
    "    ])\n",
    "imputador_knn = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('imp_knn', knn_transformer, \"occupation\"),\n",
    "    ])\n",
    "\n",
    "\n",
    "# Definimos el pipeline con el transformador\n",
    "pipeline = Pipeline([\n",
    "    ('imp_knn', imputador_knn)\n",
    "    ('valores_perdidos', imputador_modas),\n",
    "    ('preprocesador', preprocessorBasic),\n",
    "    ('classifier', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "La correlacion para  workclass_Self-emp-inc es de  0.13926915609815416\n",
      "La correlacion para  education_Bachelors es de  0.18038854662243378\n",
      "La correlacion para  education_Doctorate es de  0.13225107916695894\n",
      "La correlacion para  education_HS-grad es de  0.131111712445039\n",
      "La correlacion para  education_Masters es de  0.17440928565531122\n",
      "La correlacion para  education_Prof-school es de  0.15463439422881575\n",
      "La correlacion para  marital-status_Divorced es de  0.12689037306815928\n",
      "La correlacion para  marital-status_Married-civ-spouse es de  0.44462409597238645\n",
      "La correlacion para  marital-status_Never-married es de  0.31836383459922785\n",
      "La correlacion para  relationship_Husband es de  0.4009621086457767\n",
      "La correlacion para  relationship_Not-in-family es de  0.18841498057072167\n",
      "La correlacion para  relationship_Own-child es de  0.22843289661121388\n",
      "La correlacion para  relationship_Unmarried es de  0.14274051544312602\n",
      "La correlacion para  relationship_Wife es de  0.12309579343132912\n",
      "La correlacion para  sex_Female es de  0.2159038195491683\n",
      "La correlacion para  sex_Male es de  0.21590381954916826\n",
      "La correlacion para  Handlers-cleaners es de  0.135231792241767\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "for c in X_transformed.columns:\n",
    "    if c in X:  # No es OHE\n",
    "        print(\"\")\n",
    "    else:\n",
    "        contingency_table = pd.crosstab(X_transformed[c], y)  # Tabla de frecuencias\n",
    "\n",
    "        chi2, _, _, _ = chi2_contingency(contingency_table)  \n",
    "        n = contingency_table.sum().sum()  \n",
    "        r, k = contingency_table.shape  \n",
    "\n",
    "        if min(r, k) - 1 == 0: #Por si hay una division por cero\n",
    "            print(\"Fallo en \", c, \" división por 0\")\n",
    "            continue\n",
    "        q = min(r, k)\n",
    "        v = np.sqrt(chi2 / (n * ( q - 1)))  \n",
    "        if (v > 0.1):\n",
    "            print(\"La correlacion para \", c, \"es de \", v)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez pasadas todas las variables a numericas, podemos ver la matriz de corelacion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'binary_transformer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m X_transformed \u001b[38;5;241m=\u001b[39m \u001b[43mbinary_transformer\u001b[49m\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[0;32m      2\u001b[0m X_transformed \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([X_transformed, y], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      3\u001b[0m corr_matrix \u001b[38;5;241m=\u001b[39m X_transformed\u001b[38;5;241m.\u001b[39mcorr()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'binary_transformer' is not defined"
     ]
    }
   ],
   "source": [
    "X_transformed = binary_transformer.transform(X)\n",
    "X_transformed = pd.concat([X_transformed, y], axis=1)\n",
    "corr_matrix = X_transformed.corr()\n",
    "filtered_corr = corr_matrix.where(corr_matrix != 1)\n",
    "\n",
    "plt.figure(figsize=(60,60))\n",
    "sns.heatmap(filtered_corr, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observan varias correlaciones, algunas obvias como marital-status_Married-civ-spouse con relationship_husband y otras que vamos a analizar. Primero vamos a ver las mas influyentes en el propio resultado\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most influential factors:\n",
      " income                               1.000000\n",
      "marital-status_Married-civ-spouse    0.444696\n",
      "education-num                        0.335154\n",
      "marital-status_Never-married         0.318440\n",
      "age                                  0.234037\n",
      "hours-per-week                       0.229689\n",
      "Name: income, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "correlations = X_transformed.corr()['income'].sort_values(ascending=False)\n",
    "correlations = abs(correlations).sort_values(ascending=False)\n",
    "correlations = correlations.drop('relationship_Husband') \n",
    "correlations = correlations.drop('sex_Male')\n",
    "correlations = correlations.drop('sex_Female')\n",
    "\n",
    "\n",
    "print(\"Most influential factors:\\n\", correlations[:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probamos el rendimiento de nuetro modelo con la metrica seleccionada "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 columna\n",
      "1 columna\n",
      "age                  0\n",
      "workclass         1836\n",
      "final-weight         0\n",
      "education            0\n",
      "education-num        0\n",
      "marital-status       0\n",
      "relationship         0\n",
      "race                 0\n",
      "sex                  0\n",
      "capital-gain         0\n",
      "capital-loss         0\n",
      "hours-per-week       0\n",
      "native-country     583\n",
      "dtype: int64\n",
      "1 columna\n",
      "1 columna\n"
     ]
    }
   ],
   "source": [
    "# print(X)\n",
    "pipeline.fit(X, y)\n",
    "# print(X.isnull().sum())\n",
    "predicciones = pipeline.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7970885415067105\n",
      "Confusion Matrix:\n",
      " [[22946  1774]\n",
      " [ 4833  3008]]\n",
      "AUC: 0.6559303918206061\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", accuracy_score(y, predicciones))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y, predicciones))\n",
    "print(\"AUC:\", roc_auc_score(y, predicciones)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probamos el rendimiento de nuestro modelo con el test.data que previamente esta dividio por|1x3 Cross validator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "columns are missing: {'race', 'education', 'native-country', 'workclass', 'marital-status', 'sex', 'relationship'}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 30\u001b[0m\n\u001b[0;32m     27\u001b[0m X_test_transformed \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([X_test_transformed, occupationOHE], axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m#Todas las columnas sin nulos y en OHE\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m#Predecimos los valores con el conjunto del test\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m predicciones_test \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test_transformed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTest Accuracy:\u001b[39m\u001b[38;5;124m\"\u001b[39m, accuracy_score(y_test, predicciones_test))\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Confusion Matrix:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, confusion_matrix(y_test, predicciones_test))\n",
      "File \u001b[1;32mc:\\Users\\jorge\\anaconda3\\envs\\py311ml\\Lib\\site-packages\\sklearn\\pipeline.py:602\u001b[0m, in \u001b[0;36mPipeline.predict\u001b[1;34m(self, X, **params)\u001b[0m\n\u001b[0;32m    600\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _routing_enabled():\n\u001b[0;32m    601\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, name, transform \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter(with_final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m--> 602\u001b[0m         Xt \u001b[38;5;241m=\u001b[39m \u001b[43mtransform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    603\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mpredict(Xt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m    605\u001b[0m \u001b[38;5;66;03m# metadata routing enabled\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jorge\\anaconda3\\envs\\py311ml\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:295\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 295\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    297\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    298\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    299\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    300\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    301\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\jorge\\anaconda3\\envs\\py311ml\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:1003\u001b[0m, in \u001b[0;36mColumnTransformer.transform\u001b[1;34m(self, X, **params)\u001b[0m\n\u001b[0;32m   1001\u001b[0m     diff \u001b[38;5;241m=\u001b[39m all_names \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(column_names)\n\u001b[0;32m   1002\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m diff:\n\u001b[1;32m-> 1003\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns are missing: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdiff\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1004\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1005\u001b[0m     \u001b[38;5;66;03m# ndarray was used for fitting or transforming, thus we only\u001b[39;00m\n\u001b[0;32m   1006\u001b[0m     \u001b[38;5;66;03m# check that n_features_in_ is consistent\u001b[39;00m\n\u001b[0;32m   1007\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_n_features(X, reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mValueError\u001b[0m: columns are missing: {'race', 'education', 'native-country', 'workclass', 'marital-status', 'sex', 'relationship'}"
     ]
    }
   ],
   "source": [
    "#Quitamos el punto final\n",
    "def transformar_income_test(X): \n",
    "    X = X.copy()\n",
    "    X['income'] = X['income'].str.strip().str.rstrip('.')\n",
    "    X['income'] = X['income'].map({'<=50K': 0, '>50K': 1})\n",
    "    return X\n",
    "\n",
    "data_test = pd.read_csv('src/adult.test', header=None, skiprows=1, sep=',\\s', na_values=[\"?\"], engine='python')\n",
    "data_test.columns = ['age', 'workclass', 'final-weight', 'education', 'education-num', 'marital-status',\n",
    "                     'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss',\n",
    "                     'hours-per-week', 'native-country', 'income']\n",
    "\n",
    "data_test = transformar_income_test(data_test)\n",
    "X_test, y_test = separar_x_y(data_test)\n",
    "# X_test_transformed = binary_transformer.transform(X_test)\n",
    "\n",
    "\n",
    "#Predecimos los valores con el conjunto del test\n",
    "predicciones_test = pipeline.predict(X_test)\n",
    "print(\"\\nTest Accuracy:\", accuracy_score(y_test, predicciones_test))\n",
    "print(\"Test Confusion Matrix:\\n\", confusion_matrix(y_test, predicciones_test))\n",
    "print(\"Test AUC:\", roc_auc_score(y_test, predicciones_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

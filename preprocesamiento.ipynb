{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kernel -> py311ml\n",
    "# imports aqui ->\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import auc, roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import shap\n",
    "from shap import SamplingExplainer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, StratifiedShuffleSplit\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from scipy.stats import mannwhitneyu\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import importlib\n",
    "# import subprocess\n",
    "# import sys\n",
    "\n",
    "# def ensure_package(pkg_name):\n",
    "#     if importlib.util.find_spec(pkg_name) is None:\n",
    "#         print(f\"Instalando {pkg_name}…\")\n",
    "#         subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg_name])\n",
    "#     else:\n",
    "#         print(f\"{pkg_name} ya está instalado.\")\n",
    "\n",
    "# ensure_package(\"skrebate\")\n",
    "# from skrebate import ReliefF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeramente vamos a leer el archivo y comprobar las variables, con sus tipos de dato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ Shape ------------\n",
      "(32561, 15)\n",
      "------------ Types ------------\n",
      "age                int64\n",
      "workclass         object\n",
      "final-weight       int64\n",
      "education         object\n",
      "education-num      int64\n",
      "marital-status    object\n",
      "occupation        object\n",
      "relationship      object\n",
      "race              object\n",
      "sex               object\n",
      "capital-gain       int64\n",
      "capital-loss       int64\n",
      "hours-per-week     int64\n",
      "native-country    object\n",
      "income            object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('src/adult.data', header=None, sep=',\\s', na_values=[\"?\"], engine='python')\n",
    "data.columns = ['age', 'workclass', 'final-weight', 'education', 'education-num', 'marital-status',\n",
    "                 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss',\n",
    "                   'hours-per-week', 'native-country', 'income']\n",
    "\n",
    "\n",
    "def check_df(dataframe, head=5):\n",
    "    print(\"------------ Shape ------------\")\n",
    "    print(dataframe.shape)\n",
    "    print(\"------------ Types ------------\")\n",
    "    print(dataframe.dtypes)\n",
    "    \n",
    "check_df(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprobamos que variables contienen valores perdidos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora con ayuda de graficas, visualizamos la distribucion de las variables anteriores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def check_col_plt(df, columns):\n",
    "#     num_plots = len(columns)\n",
    "#     # Definimos el número de filas y columnas del grid. En este ejemplo usamos 2x2.\n",
    "#     rows, cols = 2, 2\n",
    "\n",
    "#     fig, axes = plt.subplots(rows, cols, figsize=(12, 8))\n",
    "#     axes = axes.flatten()  # Aplanamos la matriz de ejes para iterar fácilmente\n",
    "\n",
    "#     for i, col in enumerate(columns):\n",
    "#         ax = axes[i]\n",
    "#         # Obtenemos la cuenta de cada valor único en la columna\n",
    "#         count_data = df[col].value_counts().reset_index()\n",
    "#         count_data.columns = [col, 'count']\n",
    "\n",
    "#         # Creamos el gráfico de barras\n",
    "#         bars = ax.bar(count_data[col].astype(str), count_data['count'], color='skyblue')\n",
    "#         ax.set_title(f\"Distribución de {col}\")\n",
    "#         ax.set_xlabel(col)\n",
    "#         ax.set_ylabel(\"Conteo\")\n",
    "\n",
    "#         # Añadimos las etiquetas de conteo encima de cada barra\n",
    "#         for bar in bars:\n",
    "#             height = bar.get_height()\n",
    "#             ax.text(bar.get_x() + bar.get_width()/2, height, f'{int(height)}',\n",
    "#                     ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "#     # Eliminamos los ejes sobrantes si hay menos plots que subplots\n",
    "#     for j in range(num_plots, len(axes)):\n",
    "#         fig.delaxes(axes[j])\n",
    "    \n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "# print(data.columns)\n",
    "# # Ejemplo de uso\n",
    "# ncolumns = [\"final-weight\", \"education\", \"education-num\"]\n",
    "# check_col_plt(data, ncolumns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OUTLIERS CON MEDIANA\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class OutlierDetecion_treatment_IQR(TransformerMixin):\n",
    "\n",
    "    # Constructor de la clase\n",
    "    def __init__(self, k=1.5, columns=None):\n",
    "        self.k = k\n",
    "        self.columns = columns\n",
    "    \n",
    "    # Método fit\n",
    "    def fit(self, X, y=None):\n",
    "        # Transformamos X a DataFrame por si llega un array de Numpy (para compatibilidad en la Pipeline)\n",
    "        X = pd.DataFrame(X)\n",
    "        if self.columns == None:\n",
    "            # Si no se determinan variables en el constructor si tratan todas\n",
    "            self.columns = X.columns\n",
    "        self.stats = X.describe(percentiles=[0.25, 0.75])\n",
    "        # Devolvemos el propio objeto modificado\n",
    "        return self\n",
    "\n",
    "    # Método transform\n",
    "    def transform(self, X):\n",
    "        # Transformamos X a DataFrame por si llega un array de Numpy (para compatibilidad en la Pipeline)\n",
    "        X = pd.DataFrame(X)\n",
    "        # Creamos una copia del DataFrame X para no perder los datos originales\n",
    "        Xaux = X.copy()\n",
    "        # Se calcula el IQR de cada variable\n",
    "        # IQRs = Xaux.quantile(0.75) - Xaux.quantile(0.25)\n",
    "        IQRs = self.stats.loc[\"75%\", :] - self.stats.loc[\"25%\", :]\n",
    "        # Se calculan los límites inferiores y superiores   \n",
    "        limiteInf = self.stats.loc[\"25%\", :] - self.k * IQRs\n",
    "        limiteSup = self.stats.loc[\"75%\", :] + self.k * IQRs\n",
    "        # Se comprueba qué elementos están por encima y por debajo de dichos límites (máscaras de booleanos)  \n",
    "        menores = Xaux < limiteInf\n",
    "        mayores = Xaux > limiteSup\n",
    "        valores = np.logical_or(menores, mayores)\n",
    "        # Se recorren las variables para detectar outliers y tratarlos (sustituir por la mediana de la variable)\n",
    "        out_total = 0\n",
    "        for c in self.columns:\n",
    "            # obtenemos la lista de booleanos correspondientes a si los valores de los ejemplos son outliers o no para la variable c\n",
    "            indices = valores[c]\n",
    "            # Si hay outliers\n",
    "            if indices.any():\n",
    "                # Los sustituimos por la mediana\n",
    "                Xaux.loc[indices, c] = Xaux[c].median()\n",
    "                out_total += len(indices)\n",
    "        # Se devuelve el DataFrame modificado\n",
    "        # print(\"Total de outliers: \", out_total)\n",
    "        return Xaux\n",
    "    \n",
    "    # Método para asignar los valores de los híper-parámetros y que, de este modo, \n",
    "        # podamos aplicar GridSearchCV sobre un objeto de esta clase\n",
    "    def set_params(self, **parameters):\n",
    "        for parameter, value in parameters.items():\n",
    "            setattr(self, parameter, value)\n",
    "        return self\n",
    "    \n",
    "    # Método para obtener los valores de los híper-parámetros que queramos del modelo (lo usa GridSearchCV al mostrar la mejor configuración)\n",
    "    def get_params(self, deep=True):\n",
    "        # Devolvemos los valores de los híper-parámetros del método de preparación de datos\n",
    "        return {\"k\": self.k}\n",
    "    \n",
    "class OutlierDetecion_treatment_MeanStd(TransformerMixin):\n",
    "    # Constructor de la clase\n",
    "    def __init__(self, k=2):\n",
    "        self.k = k\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = pd.DataFrame(X)\n",
    "        self.mean = X.mean()\n",
    "        self.std = X.std()\n",
    "        self.median = X.median()\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        Xaux = X.copy()\n",
    "        out_total = 0\n",
    "        for c in X.columns:\n",
    "            # Se calculan los límites inferiores y superiores\n",
    "            limiteInf = self.mean[c] - self.k * self.std[c]\n",
    "            limiteSup = self.mean[c] + self.k * self.std[c]\n",
    "            # Se comprueba qué elementos están por encima y por debajo de dichos límites (máscaras de booleanos)\n",
    "            menores = Xaux[c] < limiteInf\n",
    "            mayores = Xaux[c] > limiteSup\n",
    "            valores = np.logical_or(menores, mayores)\n",
    "            # Si hay outliers\n",
    "            if valores.any():\n",
    "                # Los sustituimos por la mediana\n",
    "                Xaux.loc[valores, c] = self.median[c]\n",
    "                out_total += len(valores)\n",
    "        # print(\"Total de outliers: \" , out_total)\n",
    "        return Xaux\n",
    "    \n",
    "    def set_params(self, **parameters):\n",
    "        for parameter, value in parameters.items():\n",
    "            setattr(self, parameter, value)\n",
    "        return self\n",
    "    def get_params(self, deep=True):\n",
    "        return {\"k\": self.k}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OUTLIERS CON MEDIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class OutlierDetecion_treatment_IQR(TransformerMixin):\n",
    "\n",
    "    # Constructor de la clase\n",
    "    def __init__(self, k=1.5, columns=None):\n",
    "        self.k = k\n",
    "        self.columns = columns\n",
    "    \n",
    "    # Método fit\n",
    "    def fit(self, X, y=None):\n",
    "        # Transformamos X a DataFrame por si llega un array de Numpy (para compatibilidad en la Pipeline)\n",
    "        X = pd.DataFrame(X)\n",
    "        if self.columns == None:\n",
    "            # Si no se determinan variables en el constructor si tratan todas\n",
    "            self.columns = X.columns\n",
    "        self.stats = X.describe(percentiles=[0.25, 0.75])\n",
    "        # Devolvemos el propio objeto modificado\n",
    "        return self\n",
    "\n",
    "    # Método transform\n",
    "    def transform(self, X):\n",
    "        # Transformamos X a DataFrame por si llega un array de Numpy (para compatibilidad en la Pipeline)\n",
    "        X = pd.DataFrame(X)\n",
    "        # Creamos una copia del DataFrame X para no perder los datos originales\n",
    "        Xaux = X.copy()\n",
    "        # Se calcula el IQR de cada variable\n",
    "        # IQRs = Xaux.quantile(0.75) - Xaux.quantile(0.25)\n",
    "        IQRs = self.stats.loc[\"75%\", :] - self.stats.loc[\"25%\", :]\n",
    "        # Se calculan los límites inferiores y superiores   \n",
    "        limiteInf = self.stats.loc[\"25%\", :] - self.k * IQRs\n",
    "        limiteSup = self.stats.loc[\"75%\", :] + self.k * IQRs\n",
    "        # Se comprueba qué elementos están por encima y por debajo de dichos límites (máscaras de booleanos)  \n",
    "        menores = Xaux < limiteInf\n",
    "        mayores = Xaux > limiteSup\n",
    "        valores = np.logical_or(menores, mayores)\n",
    "        # Se recorren las variables para detectar outliers y tratarlos (sustituir por la mediana de la variable)\n",
    "        out_total = 0\n",
    "        for c in self.columns:\n",
    "            # obtenemos la lista de booleanos correspondientes a si los valores de los ejemplos son outliers o no para la variable c\n",
    "            indices = valores[c]\n",
    "            # Si hay outliers\n",
    "            if indices.any():\n",
    "                # Los sustituimos por la mediana\n",
    "                Xaux.loc[indices, c] = Xaux[c].mean()\n",
    "                out_total += len(indices)\n",
    "        # Se devuelve el DataFrame modificado\n",
    "        # print(\"Total de outliers: \", out_total)\n",
    "        return Xaux\n",
    "    \n",
    "    # Método para asignar los valores de los híper-parámetros y que, de este modo, \n",
    "        # podamos aplicar GridSearchCV sobre un objeto de esta clase\n",
    "    def set_params(self, **parameters):\n",
    "        for parameter, value in parameters.items():\n",
    "            setattr(self, parameter, value)\n",
    "        return self\n",
    "    \n",
    "    # Método para obtener los valores de los híper-parámetros que queramos del modelo (lo usa GridSearchCV al mostrar la mejor configuración)\n",
    "    def get_params(self, deep=True):\n",
    "        # Devolvemos los valores de los híper-parámetros del método de preparación de datos\n",
    "        return {\"k\": self.k}\n",
    "    \n",
    "class OutlierDetecion_treatment_MeanStd(TransformerMixin):\n",
    "    # Constructor de la clase\n",
    "    def __init__(self, k=2):\n",
    "        self.k = k\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = pd.DataFrame(X)\n",
    "        self.mean = X.mean()\n",
    "        self.std = X.std()\n",
    "        self.median = X.median()\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        Xaux = X.copy()\n",
    "        out_total = 0\n",
    "        for c in X.columns:\n",
    "            # Se calculan los límites inferiores y superiores\n",
    "            limiteInf = self.mean[c] - self.k * self.std[c]\n",
    "            limiteSup = self.mean[c] + self.k * self.std[c]\n",
    "            # Se comprueba qué elementos están por encima y por debajo de dichos límites (máscaras de booleanos)\n",
    "            menores = Xaux[c] < limiteInf\n",
    "            mayores = Xaux[c] > limiteSup\n",
    "            valores = np.logical_or(menores, mayores)\n",
    "            # Si hay outliers\n",
    "            if valores.any():\n",
    "                # Los sustituimos por la mediana\n",
    "                Xaux.loc[valores, c] = self.mean[c]\n",
    "                out_total += len(valores)\n",
    "        # print(\"Total de outliers: \" , out_total)\n",
    "        return Xaux\n",
    "    \n",
    "    def set_params(self, **parameters):\n",
    "        for parameter, value in parameters.items():\n",
    "            setattr(self, parameter, value)\n",
    "        return self\n",
    "    def get_params(self, deep=True):\n",
    "        return {\"k\": self.k}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OUTLIERS CON LIMITE SUPERIOR - INFERIOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutlierDetecion_treatment_IQR(TransformerMixin):\n",
    "\n",
    "    # Constructor de la clase\n",
    "    def __init__(self, k=1.5, columns=None):\n",
    "        self.k = k\n",
    "        self.columns = columns\n",
    "    \n",
    "    # Método fit\n",
    "    def fit(self, X, y=None):\n",
    "        # Transformamos X a DataFrame por si llega un array de Numpy (para compatibilidad en la Pipeline)\n",
    "        X = pd.DataFrame(X)\n",
    "        if self.columns == None:\n",
    "            # Si no se determinan variables en el constructor si tratan todas\n",
    "            self.columns = X.columns\n",
    "        self.stats = X.describe(percentiles=[0.25, 0.75])\n",
    "        # Devolvemos el propio objeto modificado\n",
    "        return self\n",
    "\n",
    "    # Método transform\n",
    "    def transform(self, X):\n",
    "        X = pd.DataFrame(X)\n",
    "        Xaux = X.copy()\n",
    "\n",
    "        # Estadísticos guardados en fit\n",
    "        IQRs      = self.stats.loc[\"75%\", :] - self.stats.loc[\"25%\", :]\n",
    "        limiteInf = self.stats.loc[\"25%\", :] - self.k * IQRs\n",
    "        limiteSup = self.stats.loc[\"75%\", :] + self.k * IQRs\n",
    "\n",
    "        # Para cada columna, winsorizamos\n",
    "        for c in self.columns:\n",
    "            mask_low  = Xaux[c] < limiteInf[c]\n",
    "            mask_high = Xaux[c] > limiteSup[c]\n",
    "            if mask_low.any():\n",
    "                Xaux.loc[mask_low,  c] = limiteInf[c]\n",
    "            if mask_high.any():\n",
    "                Xaux.loc[mask_high, c] = limiteSup[c]\n",
    "\n",
    "        return Xaux\n",
    "    \n",
    "    # Método para asignar los valores de los híper-parámetros y que, de este modo, \n",
    "        # podamos aplicar GridSearchCV sobre un objeto de esta clase\n",
    "    def set_params(self, **parameters):\n",
    "        for parameter, value in parameters.items():\n",
    "            setattr(self, parameter, value)\n",
    "        return self\n",
    "    \n",
    "    # Método para obtener los valores de los híper-parámetros que queramos del modelo (lo usa GridSearchCV al mostrar la mejor configuración)\n",
    "    def get_params(self, deep=True):\n",
    "        # Devolvemos los valores de los híper-parámetros del método de preparación de datos\n",
    "        return {\"k\": self.k}\n",
    "    \n",
    "    from sklearn.base import TransformerMixin\n",
    "\n",
    "class OutlierDetecion_treatment_MeanStd(TransformerMixin):\n",
    "    def __init__(self, k=2):\n",
    "        self.k = k\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = pd.DataFrame(X)\n",
    "        # Calculamos y guardamos media y desviación por columna\n",
    "        self.mean_ = X.mean()\n",
    "        self.std_  = X.std()\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        Xaux = pd.DataFrame(X).copy()\n",
    "        # Cálculo de límites por columna\n",
    "        limiteInf = self.mean_ - self.k * self.std_\n",
    "        limiteSup = self.mean_ + self.k * self.std_\n",
    "        \n",
    "        # Winsorización\n",
    "        for c in Xaux.columns:\n",
    "            # Máscaras\n",
    "            mask_low  = Xaux[c] < limiteInf[c]\n",
    "            mask_high = Xaux[c] > limiteSup[c]\n",
    "            # Sustitución por límites\n",
    "            if mask_low.any():\n",
    "                Xaux.loc[mask_low,  c] = limiteInf[c]\n",
    "            if mask_high.any():\n",
    "                Xaux.loc[mask_high, c] = limiteSup[c]\n",
    "        \n",
    "        return Xaux\n",
    "    \n",
    "    def set_params(self, **parameters):\n",
    "        for parameter, value in parameters.items():\n",
    "            setattr(self, parameter, value)\n",
    "        return self\n",
    "    \n",
    "    def get_params(self, deep=True):\n",
    "        return {\"k\": self.k}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version para usar con column transformer y meter este paso en la pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rellenarModa(data):\n",
    "    dataCopiada = data.copy()\n",
    "    # Si 'columnas' no es una lista, se convierte en una lista\n",
    "    columnas = dataCopiada.columns[dataCopiada.isnull().any()].tolist()\n",
    "    for columna in columnas:\n",
    "        if (columna != \"occupation\"):\n",
    "        # Calcula la moda de la columna y rellena los valores nulos\n",
    "            moda = dataCopiada[columna].mode(dropna = True)\n",
    "            dataCopiada[columna] = dataCopiada[columna].fillna(moda)\n",
    "    return dataCopiada\n",
    "\n",
    "imputador_modas = FunctionTransformer(rellenarModa, validate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rellenarKnn(data):\n",
    "    #Columna a imputar\n",
    "    columna = \"occupation\"\n",
    "\n",
    "    # 2. Crear una copia del DataFrame\n",
    "    dataCopiada = data.copy()\n",
    "\n",
    "    # 3. Guardar las filas con y sin valores faltantes\n",
    "    #df_completo = df_copy[df_copy[columna].notna()]\n",
    "    #df_faltante = df_copy[df_copy[columna].isna()]\n",
    "\n",
    "    # 4. Codificar la columna objetivo con LabelEncoder\n",
    "    le_obj = LabelEncoder()\n",
    "    dataCopiada[columna] = dataCopiada[columna].astype(str)\n",
    "    dataCopiada[columna] = le_obj.fit_transform(dataCopiada[columna])\n",
    "\n",
    "    # 5. Codificar otras columnas categóricas (si hay), necesarias para el imputador\n",
    "    label_encoders = {}\n",
    "    for col in dataCopiada.columns:\n",
    "        if dataCopiada[col].dtype == 'object' and col != columna:\n",
    "            le = LabelEncoder()\n",
    "            dataCopiada[col] = dataCopiada[col].astype(str)\n",
    "            dataCopiada[col] = le.fit_transform(dataCopiada[col])\n",
    "            label_encoders[col] = le\n",
    "\n",
    "# 6. Crear subconjunto de columnas para imputación\n",
    "# (puede ser todo menos la columna objetivo o solo algunas relevantes)\n",
    "    columnasPred = dataCopiada.columns[dataCopiada.columns != columna]\n",
    "\n",
    "    # 7. Juntar columna objetivo + predictoras\n",
    "    df_model = dataCopiada[[columna] + list(columnasPred)]\n",
    "\n",
    "    # 8. Aplicar KNNImputer solo a esta matriz\n",
    "    imputer = KNNImputer(n_neighbors=5)\n",
    "    dataImputada = pd.DataFrame(imputer.fit_transform(df_model), columns=df_model.columns)\n",
    "\n",
    "    # 9. Redondear y reconvertir la columna objetivo\n",
    "    dataImputada[columna] = dataImputada[columna].astype(int)\n",
    "    dataImputada[columna] = le_obj.inverse_transform(dataImputada[columna])\n",
    "\n",
    "    # 10. Insertar la columna imputada en el DataFrame original\n",
    "    dataFinal = data.copy()\n",
    "    dataFinal[columna] = dataImputada[columna]\n",
    "    # print(dataFinal.isnull().sum())\n",
    "    return dataFinal\n",
    "\n",
    "imputador_knn = FunctionTransformer(rellenarKnn, validate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7857417076167076\n",
      "Confusion Matrix:\n",
      " [[15519  4276]\n",
      " [ 1305  4948]]\n",
      "AUC: 0.7876430154647265\n",
      "AUC en Val : 0.7728928256338784\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def transformar_income(X): \n",
    "    \"\"\"Transforma la columna 'income' en una variable binaria.\"\"\"\n",
    "\n",
    "    X = X.copy()\n",
    "    X['income'] = X['income'].str.strip().map({'<=50K': 0, '>50K': 1})\n",
    "    return X\n",
    "\n",
    "def separar_x_y(data):\n",
    "    X = data.drop('income', axis=1)\n",
    "    y = data['income']\n",
    "    return X, y\n",
    "\n",
    "def separar_x_y_col(data, col):\n",
    "    X = data.drop(col, axis=1)\n",
    "    y = data[col]\n",
    "    return X, y\n",
    "\n",
    "def transformar_categoricas_a_binarias(X): \n",
    "\n",
    "    X = X.copy()\n",
    "    variables_categoricas_binarias = X.select_dtypes(include=['object']).columns\n",
    "    dummies = pd.get_dummies(X, columns=variables_categoricas_binarias)\n",
    "    if hasattr(transformar_categoricas_a_binarias, 'dummy_columns'):\n",
    "        dummies = dummies.reindex(columns=transformar_categoricas_a_binarias.dummy_columns, fill_value=0)\n",
    "    else:\n",
    "        transformar_categoricas_a_binarias.dummy_columns = dummies.columns\n",
    "    return dummies\n",
    "def crear_sex_country(X):\n",
    "    X = X.copy()\n",
    "    X[\"sex-native-country\"] = X[\"sex\"] + \"-\" + X[\"native-country\"]\n",
    "    X = eliminar_columna(X, 'sex')\n",
    "    X = eliminar_columna(X, \"native-country\")\n",
    "    return X\n",
    "\n",
    "def crear_capital_balance(X):\n",
    "    X = X.copy()\n",
    "    X['capital-balance'] = np.sqrt(X['capital-gain'] - X['capital-loss'])\n",
    "    # X.drop(columns=['capital-gain', 'capital-loss'])\n",
    "    return X\n",
    "\n",
    "def log_numerical(X, columna):\n",
    "    X = X.copy()\n",
    "    nueva_col = \"log-\" + columna\n",
    "    X[nueva_col] = np.log(X[columna] + 1)\n",
    "    return X\n",
    "\n",
    "def log_capital_gain_loss_finalWeight(X):\n",
    "    X = X.copy()\n",
    "    X = log_numerical(X, 'capital-gain')\n",
    "    X = log_numerical(X, 'capital-loss')\n",
    "    X = log_numerical(X, 'final-weight')\n",
    "    return X\n",
    "\n",
    "def knnImp(X, y):\n",
    "    y = y.copy()\n",
    "    indicesNulos = np.where(y == -1)[0] \n",
    "    indicesNoNulos = np.where(y != -1)[0] \n",
    "    X_train = X.iloc[indicesNoNulos]\n",
    "    X_test = X.iloc[indicesNulos]\n",
    "    y_train = y[indicesNoNulos]\n",
    "\n",
    "    \n",
    "    knn = KNeighborsClassifier(n_neighbors= 3)\n",
    "    knn.fit(X_train, y_train)\n",
    "    pred = knn.predict(X_test)\n",
    "    y[indicesNulos] = pred\n",
    "    return y\n",
    "\n",
    "def eliminar_columna(data, col):\n",
    "    if not isinstance(data, pd.DataFrame):\n",
    "        data = pd.DataFrame(data)\n",
    "    return data.drop(columns=[col], errors=\"ignore\")\n",
    "\n",
    "def mannwhitney_score(X, y):\n",
    "    \"\"\"\n",
    "    Para cada columna en X (asumiendo problema binario):\n",
    "      - calcula mannwhitneyu entre los dos grupos de y\n",
    "      - devuelve (estadísticos U, p-valores)\n",
    "    \"\"\"\n",
    "    classes = np.unique(y)\n",
    "    if len(classes) != 2:\n",
    "        raise ValueError(\"Mann-Whitney sólo para problemas con 2 clases\")\n",
    "    \n",
    "    n_feats = X.shape[1]\n",
    "    stats = np.zeros(n_feats)\n",
    "    pvals = np.ones(n_feats)\n",
    "    \n",
    "    for i in range(n_feats):\n",
    "        xi = X[:, i]\n",
    "        # extraemos los dos grupos\n",
    "        grp0 = xi[y == classes[0]]\n",
    "        grp1 = xi[y == classes[1]]\n",
    "        stat, p = mannwhitneyu(grp0, grp1, alternative='two-sided')\n",
    "        stats[i] = stat\n",
    "        pvals[i] = p\n",
    "    \n",
    "    return stats, pvals\n",
    "\n",
    "binary_transformer = FunctionTransformer(transformar_categoricas_a_binarias)\n",
    "eliminar_relationship = FunctionTransformer(eliminar_columna, kw_args={\"col\": [\"relationship\"]}, validate=False)\n",
    "eliminar_marital_status = FunctionTransformer(eliminar_columna, kw_args={\"col\": [\"marital-status\"]}, validate=False)\n",
    "eliminar_education = FunctionTransformer(eliminar_columna, kw_args={\"col\": [\"education\"]}, validate=False)\n",
    "sex_country = FunctionTransformer(crear_sex_country, validate=False)\n",
    "crear_col_capital_balance = FunctionTransformer(crear_capital_balance, validate=False)\n",
    "crear_col_log_numerical = FunctionTransformer(log_capital_gain_loss_finalWeight, validate = False)\n",
    "\n",
    "data_binary_income = transformar_income(data)\n",
    "X, X_val = train_test_split(data_binary_income, test_size=0.2, random_state=34)\n",
    "X, y = separar_x_y(X)\n",
    "X_val, y_val = separar_x_y(X_val)\n",
    "\n",
    "# X, occupation = separar_x_y_col(X, \"occupation\")\n",
    "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_features = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "# X_transformed = binary_transformer.transform(X)\n",
    "# occupationNombre = occupation.copy()\n",
    "# valoresOccupation = occupationNombre.dropna()\n",
    "# colConNulos = pd.Categorical(occupation, ordered= True).codes #Paso la variable a ordinal \n",
    "\n",
    "# occupation_ord = knnImp(X_transformed, colConNulos) #Imputo los valores\n",
    "# occupation_series = pd.Series(occupation_ord)\n",
    "# occupation_mapeados = occupation_series.map(lambda x: occupationNombre[x])\n",
    "\n",
    "# occupationOHE = pd.get_dummies(occupation_mapeados)\n",
    "# X_transformed = pd.concat([X_transformed, occupationOHE], axis = 1) #Todas las columnas sin nulos y en OHE\n",
    "\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    # ('outlier_detection', OutlierDetecion_treatment_MeanStd(k=3)),\n",
    "    ('outlier_detection', OutlierDetecion_treatment_IQR(k=5)),\n",
    "    # ('crear_capital_balance', crear_col_capital_balance),\n",
    "    # ('log_capital_gain_loss_final_weight', crear_col_log_numerical)\n",
    "    # ('scaler', RobustScaler())\n",
    "    # ('scaler', StandardScaler()), \n",
    "    # ('scaler', MinMaxScaler()) Descartado despues de las pruebas\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    # ('eliminar_variable_relationship', eliminar_relationship),\n",
    "    # ('eliminar_variable_relationship', eliminar_education),\n",
    "    # ('crear_sex_country', sex_country), #Aumenta un poco la carga computacional\n",
    "    ('one_hot_encoding', OneHotEncoder(sparse_output=False, handle_unknown='ignore')),\n",
    "])\n",
    "\n",
    "preprocessorBasic = ColumnTransformer(\n",
    "    transformers=[\n",
    "        # ('num', numeric_transformer, numeric_features), \n",
    "        ('cat', categorical_transformer, categorical_features),\n",
    "    ])\n",
    "\n",
    "# Definimos el pipeline con el transformador\n",
    "pipeline = Pipeline([\n",
    "    ('valores_perdidos',     imputador_modas),\n",
    "    # ('ocupation_perdidos', imputador_knn),\n",
    "\n",
    "    ('preprocesador',  preprocessorBasic),\n",
    "\n",
    "    # ('anova',        SelectKBest(score_func=f_classif, k=50)),\n",
    "    # ('man-white',    SelectKBest(score_func=mannwhitney_score, k=50)),\n",
    "    # ('relief',       ReliefF(n_neighbors=10, n_features_to_select=50)), Demasiado costoso computacionalmente\n",
    "    ('wrapper',      RFE(estimator=LogisticRegression(), n_features_to_select=25, step=3)), # Seleccionado como el mejor metodo de seleccion\n",
    "\n",
    "    ('classifier',   KNeighborsClassifier())\n",
    "    # ('classifier', DecisionTreeClassifier())\n",
    "])\n",
    "\n",
    "\n",
    "pipeline.fit(X, y.to_numpy())\n",
    "predicciones = pipeline.predict(X)\n",
    "print(\"Accuracy:\", accuracy_score(y, predicciones))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y, predicciones))\n",
    "print(\"AUC:\", roc_auc_score(y, predicciones))\n",
    "print(\"AUC en Val :\", roc_auc_score(y_val, pipeline.predict(X_val))) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre = pipeline.named_steps['preprocesador']\n",
    "# anova = pipeline.named_steps['anova']\n",
    "\n",
    "# # 3. Obtén los nombres de todas las features tras el ColumnTransformer\n",
    "# #    (desde sklearn ≥1.0 puedes usar get_feature_names_out directamente)\n",
    "# feature_names = pre.get_feature_names_out()\n",
    "\n",
    "# # 4. Número total de features tras OHE (y demás transformaciones numéricas)\n",
    "# total_feats = len(feature_names)\n",
    "# print(f\"Features tras preprocesado: {total_feats}\")\n",
    "\n",
    "# # 5. Máscara booleana de las que seleccionó ANOVA\n",
    "# mask_sel = anova.get_support()\n",
    "\n",
    "# # 6. Número de features tras ANOVA\n",
    "# selected_feats = mask_sel.sum()\n",
    "# print(f\"Features tras ANOVA (k={anova.k}): {selected_feats}\")\n",
    "\n",
    "# # 7. (Opcional) Lista de nombres finales\n",
    "# final_names = feature_names[mask_sel]\n",
    "# print(\"Nombres de las variables seleccionadas:\\n\", final_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GridSearch CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 1944 candidates, totalling 1944 fits\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid parameter 'class_weight' for estimator KNeighborsClassifier(). Valid parameters are: ['algorithm', 'leaf_size', 'metric', 'metric_params', 'n_jobs', 'n_neighbors', 'p', 'weights'].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"c:\\Users\\jorge\\anaconda3\\envs\\py311ml\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 463, in _process_worker\n    r = call_item()\n        ^^^^^^^^^^^\n  File \"c:\\Users\\jorge\\anaconda3\\envs\\py311ml\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 291, in __call__\n    return self.fn(*self.args, **self.kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\jorge\\anaconda3\\envs\\py311ml\\Lib\\site-packages\\joblib\\parallel.py\", line 598, in __call__\n    return [func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\jorge\\anaconda3\\envs\\py311ml\\Lib\\site-packages\\joblib\\parallel.py\", line 598, in <listcomp>\n    return [func(*args, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\jorge\\anaconda3\\envs\\py311ml\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 129, in __call__\n    return self.function(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\jorge\\anaconda3\\envs\\py311ml\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 883, in _fit_and_score\n    estimator = estimator.set_params(**clone(parameters, safe=False))\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\jorge\\anaconda3\\envs\\py311ml\\Lib\\site-packages\\sklearn\\pipeline.py\", line 239, in set_params\n    self._set_params(\"steps\", **kwargs)\n  File \"c:\\Users\\jorge\\anaconda3\\envs\\py311ml\\Lib\\site-packages\\sklearn\\utils\\metaestimators.py\", line 71, in _set_params\n    super().set_params(**params)\n  File \"c:\\Users\\jorge\\anaconda3\\envs\\py311ml\\Lib\\site-packages\\sklearn\\base.py\", line 291, in set_params\n    valid_params[key].set_params(**sub_params)\n  File \"c:\\Users\\jorge\\anaconda3\\envs\\py311ml\\Lib\\site-packages\\sklearn\\base.py\", line 279, in set_params\n    raise ValueError(\nValueError: Invalid parameter 'class_weight' for estimator KNeighborsClassifier(). Valid parameters are: ['algorithm', 'leaf_size', 'metric', 'metric_params', 'n_jobs', 'n_neighbors', 'p', 'weights'].\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 40\u001b[0m\n\u001b[0;32m     37\u001b[0m data_binary_income \u001b[38;5;241m=\u001b[39m transformar_income(data)\n\u001b[0;32m     38\u001b[0m X, y \u001b[38;5;241m=\u001b[39m separar_x_y(data_binary_income)\n\u001b[1;32m---> 40\u001b[0m \u001b[43mgrid\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMejores hiperparámetros:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(grid\u001b[38;5;241m.\u001b[39mbest_params_)\n",
      "File \u001b[1;32mc:\\Users\\jorge\\anaconda3\\envs\\py311ml\\Lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jorge\\anaconda3\\envs\\py311ml\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:970\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    964\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m    965\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    966\u001b[0m     )\n\u001b[0;32m    968\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 970\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    973\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    974\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\jorge\\anaconda3\\envs\\py311ml\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1527\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1525\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1526\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1527\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jorge\\anaconda3\\envs\\py311ml\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:916\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    908\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    909\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    910\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    911\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    912\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    913\u001b[0m         )\n\u001b[0;32m    914\u001b[0m     )\n\u001b[1;32m--> 916\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    918\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    920\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    922\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    924\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    925\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    926\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    929\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    930\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    931\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    932\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    934\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    935\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    936\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    937\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    938\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    939\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\jorge\\anaconda3\\envs\\py311ml\\Lib\\site-packages\\sklearn\\utils\\parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     66\u001b[0m )\n\u001b[1;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jorge\\anaconda3\\envs\\py311ml\\Lib\\site-packages\\joblib\\parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "File \u001b[1;32mc:\\Users\\jorge\\anaconda3\\envs\\py311ml\\Lib\\site-packages\\joblib\\parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jorge\\anaconda3\\envs\\py311ml\\Lib\\site-packages\\joblib\\parallel.py:1754\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_retrieval():\n\u001b[0;32m   1748\u001b[0m \n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;66;03m# If the callback thread of a worker has signaled that its task\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m     \u001b[38;5;66;03m# triggered an exception, or if the retrieval loop has raised an\u001b[39;00m\n\u001b[0;32m   1751\u001b[0m     \u001b[38;5;66;03m# exception (e.g. `GeneratorExit`), exit the loop and surface the\u001b[39;00m\n\u001b[0;32m   1752\u001b[0m     \u001b[38;5;66;03m# worker traceback.\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aborting:\n\u001b[1;32m-> 1754\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_error_fast\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1755\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   1757\u001b[0m     \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m     \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jorge\\anaconda3\\envs\\py311ml\\Lib\\site-packages\\joblib\\parallel.py:1789\u001b[0m, in \u001b[0;36mParallel._raise_error_fast\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1785\u001b[0m \u001b[38;5;66;03m# If this error job exists, immediately raise the error by\u001b[39;00m\n\u001b[0;32m   1786\u001b[0m \u001b[38;5;66;03m# calling get_result. This job might not exists if abort has been\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m \u001b[38;5;66;03m# called directly or if the generator is gc'ed.\u001b[39;00m\n\u001b[0;32m   1788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_job \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1789\u001b[0m     \u001b[43merror_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jorge\\anaconda3\\envs\\py311ml\\Lib\\site-packages\\joblib\\parallel.py:745\u001b[0m, in \u001b[0;36mBatchCompletionCallBack.get_result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    739\u001b[0m backend \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel\u001b[38;5;241m.\u001b[39m_backend\n\u001b[0;32m    741\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39msupports_retrieve_callback:\n\u001b[0;32m    742\u001b[0m     \u001b[38;5;66;03m# We assume that the result has already been retrieved by the\u001b[39;00m\n\u001b[0;32m    743\u001b[0m     \u001b[38;5;66;03m# callback thread, and is stored internally. It's just waiting to\u001b[39;00m\n\u001b[0;32m    744\u001b[0m     \u001b[38;5;66;03m# be returned.\u001b[39;00m\n\u001b[1;32m--> 745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_return_or_raise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;66;03m# For other backends, the main thread needs to run the retrieval step.\u001b[39;00m\n\u001b[0;32m    748\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\jorge\\anaconda3\\envs\\py311ml\\Lib\\site-packages\\joblib\\parallel.py:763\u001b[0m, in \u001b[0;36mBatchCompletionCallBack._return_or_raise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    762\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m TASK_ERROR:\n\u001b[1;32m--> 763\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[0;32m    764\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[0;32m    765\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid parameter 'class_weight' for estimator KNeighborsClassifier(). Valid parameters are: ['algorithm', 'leaf_size', 'metric', 'metric_params', 'n_jobs', 'n_neighbors', 'p', 'weights']."
     ]
    }
   ],
   "source": [
    "param_grid_knn = {\n",
    "    'classifier__n_neighbors' : [10, 50, 100],\n",
    "    'classifier__weights'     : ['uniform', 'distance'],\n",
    "    'classifier__metric'      : ['euclidean', 'manhattan'],\n",
    "}\n",
    "param_grid_tree = {\n",
    "    'classifier__max_depth' : [10, 20, 30],\n",
    "    'classifier__min_samples_split' : [10, 20, 50],\n",
    "    'classifier__min_samples_leaf' : [1, 2, 4],\n",
    "    'classifier__max_features' : ['sqrt', 'log2'],\n",
    "    'classifier__criterion' : ['gini', 'entropy'],\n",
    "    'classifier__class_weight' : ['balanced', None],\n",
    "    'classifier__min_weight_fraction_leaf' : [0.0, 0.1, 0.2],\n",
    "    # 'classifier__ccp_alpha' : [0.0, 0.01, 0.1],\n",
    "    'classifier__min_impurity_decrease' : [0.0, 0.1, 0.2],    \n",
    "}    \n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 3.  Configuración del GridSearch ------------------------------------\n",
    "# --------------------------------------------------------------------\n",
    "holdout = StratifiedShuffleSplit(\n",
    "    n_splits=1, test_size=0.2, random_state=42)  # 80 % / 20 %\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=param_grid_tree,\n",
    "    # param_grid=param_grid_knn,\n",
    "    cv=holdout,\n",
    "    scoring='roc_auc',         # o 'roc_auc', 'balanced_accuracy', etc.\n",
    "    n_jobs=-1,                  # paraliza en todos los cores disponibles\n",
    "    verbose=2,                  # para ver progreso\n",
    ")\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 4.  Ajuste -----------------------------------------------------------\n",
    "# --------------------------------------------------------------------\n",
    "data_binary_income = transformar_income(data)\n",
    "X, y = separar_x_y(data_binary_income)\n",
    "\n",
    "grid.fit(X, y)\n",
    "\n",
    "print(\"Mejores hiperparámetros:\")\n",
    "print(grid.best_params_)\n",
    "print(f\"Mejor media de CV: {grid.best_score_:.4f}\")\n",
    "\n",
    "# El modelo entrenado con los mejores parámetros:\n",
    "best_model = grid.best_estimator_\n",
    "\n",
    "best = grid.best_params_\n",
    "clf_params = {\n",
    "    key.split('__', 1)[1]: value\n",
    "    for key, value in best.items()\n",
    "    if key.startswith('classifier__')\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid para wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor combo: {'wrapper__n_features_to_select': 25, 'wrapper__step': 3}\n",
      "Score CV: 0.8487307653245932\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'wrapper__n_features_to_select': [20,25,30, 35, 40],\n",
    "    'wrapper__step': [1, 3, 5]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    estimator=pipeline,  # o pipeline_rfecv\n",
    "    param_grid=param_grid,\n",
    "    cv=holdout,\n",
    "    scoring='roc_auc',\n",
    "    refit=True,\n",
    "    n_jobs=-1\n",
    ")\n",
    "grid.fit(X, y)\n",
    "\n",
    "print(\"Mejor combo:\", grid.best_params_)\n",
    "print(\"Score CV:\", grid.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from scipy.stats import chi2_contingency\n",
    "\n",
    "# for c in X_transformed.columns:\n",
    "#     if c in X:  # No es OHE\n",
    "#         print(\"\")\n",
    "#     else:\n",
    "#         contingency_table = pd.crosstab(X_transformed[c], y)  # Tabla de frecuencias\n",
    "\n",
    "#         chi2, _, _, _ = chi2_contingency(contingency_table)  \n",
    "#         n = contingency_table.sum().sum()  \n",
    "#         r, k = contingency_table.shape  \n",
    "\n",
    "#         if min(r, k) - 1 == 0: #Por si hay una division por cero\n",
    "#             print(\"Fallo en \", c, \" división por 0\")\n",
    "#             continue\n",
    "#         q = min(r, k)\n",
    "#         v = np.sqrt(chi2 / (n * ( q - 1)))  \n",
    "#         if (v > 0.1):\n",
    "#             print(\"La correlacion para \", c, \"es de \", v)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez pasadas todas las variables a numericas, podemos ver la matriz de corelacion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_transformed = binary_transformer.transform(X)\n",
    "# X_transformed = pd.concat([X_transformed, y], axis=1)\n",
    "# corr_matrix = X_transformed.corr()\n",
    "# filtered_corr = corr_matrix.where(corr_matrix != 1)\n",
    "\n",
    "# plt.figure(figsize=(60,60))\n",
    "# sns.heatmap(filtered_corr, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observan varias correlaciones, algunas obvias como marital-status_Married-civ-spouse con relationship_husband y otras que vamos a analizar. Primero vamos a ver las mas influyentes en el propio resultado\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlations = X_transformed.corr()['income'].sort_values(ascending=False)\n",
    "# correlations = abs(correlations).sort_values(ascending=False)\n",
    "# correlations = correlations.drop('relationship_Husband') \n",
    "# correlations = correlations.drop('sex_Male')\n",
    "# correlations = correlations.drop('sex_Female')\n",
    "\n",
    "\n",
    "# print(\"Most influential factors:\\n\", correlations[:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probamos el rendimiento de nuetro modelo con la metrica seleccionada "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9999385768250361\n",
      "Confusion Matrix:\n",
      " [[24720     0]\n",
      " [    2  7839]]\n",
      "AUC: 0.9998724652467798\n"
     ]
    }
   ],
   "source": [
    "predicciones = pipeline.predict(X)\n",
    "print(\"Accuracy:\", accuracy_score(y, predicciones))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y, predicciones))\n",
    "print(\"AUC:\", roc_auc_score(y, predicciones)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probamos el rendimiento de nuestro modelo con el test.data que previamente esta dividio por|1x3 Cross validator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Accuracy: 0.7770407223143542\n",
      "Test Confusion Matrix:\n",
      " [[10612  1823]\n",
      " [ 1807  2039]]\n",
      "Test AUC: 0.6917794371605985\n"
     ]
    }
   ],
   "source": [
    "#Quitamos el punto final\n",
    "def transformar_income_test(X): \n",
    "    X = X.copy()\n",
    "    X['income'] = X['income'].str.strip().str.rstrip('.')\n",
    "    X['income'] = X['income'].map({'<=50K': 0, '>50K': 1})\n",
    "    return X\n",
    "\n",
    "data_test = pd.read_csv('src/adult.test', header=None, skiprows=1, sep=',\\s', na_values=[\"?\"], engine='python')\n",
    "data_test.columns = ['age', 'workclass', 'final-weight', 'education', 'education-num', 'marital-status',\n",
    "                     'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss',\n",
    "                     'hours-per-week', 'native-country', 'income']\n",
    "\n",
    "data_test = transformar_income_test(data_test)\n",
    "X_test, y_test = separar_x_y(data_test)\n",
    "# X_test_transformed = binary_transformer.transform(X_test)\n",
    "\n",
    "\n",
    "#Predecimos los valores con el conjunto del test\n",
    "predicciones_test = pipeline.predict(X_test)\n",
    "print(\"\\nTest Accuracy:\", accuracy_score(y_test, predicciones_test))\n",
    "print(\"Test Confusion Matrix:\\n\", confusion_matrix(y_test, predicciones_test))\n",
    "print(\"Test AUC:\", roc_auc_score(y_test, predicciones_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SHAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Globales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. DataFrames de referencia\n",
    "feature_names = X.columns\n",
    "\n",
    "# 2. Función envoltorio\n",
    "def model_predict(data):\n",
    "    \"\"\"Asegura que el Pipeline recibe un DataFrame con los nombres correctos.\"\"\"\n",
    "    if not isinstance(data, pd.DataFrame):\n",
    "        data = pd.DataFrame(data, columns=feature_names)\n",
    "    return pipeline.predict_proba(data)[:, 1]\n",
    "\n",
    "# 3. Background y muestra de evaluación\n",
    "background = shap.sample(X, 200, random_state=1)     # o shap.kmeans(X, 100)\n",
    "X_eval     = shap.sample(X, 500, random_state=1)\n",
    "\n",
    "# 4. Explainer (Kernel o Sampling)\n",
    "explainer   = shap.KernelExplainer(model_predict, background)\n",
    "# explainer = shap.SamplingExplainer(model_predict, background)  # algo más rápido\n",
    "\n",
    "shap_values = explainer.shap_values(X_eval, nsamples=200)\n",
    "\n",
    "# 5. Gráfico\n",
    "shap.summary_plot(shap_values, X_eval)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Individuales "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. Solo una vez por notebook\n",
    "shap.initjs()\n",
    "\n",
    "# 1. Wrapper para el pipeline  ─ convierte ndarray→DataFrame si hace falta\n",
    "feature_names = X.columns          # guarda los nombres originales\n",
    "\n",
    "def model_predict(data):\n",
    "    if not isinstance(data, pd.DataFrame):\n",
    "        data = pd.DataFrame(data, columns=feature_names)\n",
    "    return pipeline.predict_proba(data)\n",
    "\n",
    "# 2. Background pequeño (muestra o k‑means)\n",
    "background = shap.sample(X, 200, random_state=0)      # o shap.kmeans(X, 100)\n",
    "\n",
    "# 3. Explainer (Kernel o Sampling) usando la *función*, no el pipeline\n",
    "explainer = shap.KernelExplainer(model_predict, background)\n",
    "# explainer = shap.SamplingExplainer(model_predict, background)  # 10 × más rápido\n",
    "\n",
    "# 4. Fila a explicar\n",
    "instance = X.iloc[[1]]             # DataFrame de 1 fila\n",
    "\n",
    "# 5. Cálculo limitado de coaliciones\n",
    "shap_values = explainer.shap_values(instance, nsamples=200)\n",
    "\n",
    "# 6. Force plot (clase positiva = 1)\n",
    "shap.force_plot(\n",
    "    explainer.expected_value[1],\n",
    "    shap_values[0,:,0],             # clase 1, primera fila\n",
    "    instance\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
